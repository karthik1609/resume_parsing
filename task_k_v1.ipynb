{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import textract\n",
    "import os\n",
    "from spacy.matcher import Matcher\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T07:29:35.826938Z",
     "iopub.status.busy": "2022-03-21T07:29:35.826755Z",
     "iopub.status.idle": "2022-03-21T07:29:35.830489Z",
     "shell.execute_reply": "2022-03-21T07:29:35.830116Z",
     "shell.execute_reply.started": "2022-03-21T07:29:35.826916Z"
    },
    "tags": []
   },
   "source": [
    "resumes_dict = {\n",
    "    'resume_id':\n",
    "    {                  #filename for now but to be replaced with unique id\n",
    "        'name': ('first name', 'last name'),\n",
    "        'email': ('email', 'position_tuple'),\n",
    "        'phone': ('phone', 'position_tuple'),\n",
    "        'dob': ('dob', 'position_tuple'),\n",
    "        'address': ('address', 'position_tuple'),\n",
    "        'skills': [('skill1', 'position_tuple1'), ('skill2', 'position_tuple2')],\n",
    "    }                     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(os.getcwd(), \"/home/anudeepadi/Documents/Fw__Sample_resumes_\", \"*.docx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/anudeepadi/Documents/Fw__Sample_resumes_/Resume1_UI.docx',\n",
       " '/home/anudeepadi/Documents/Fw__Sample_resumes_/Resume-Dileep2020 (1).docx',\n",
       " '/home/anudeepadi/Documents/Fw__Sample_resumes_/Resume_11_Dotnet_Developer_.docx',\n",
       " '/home/anudeepadi/Documents/Fw__Sample_resumes_/resume_8_service_now.docx',\n",
       " '/home/anudeepadi/Documents/Fw__Sample_resumes_/Resume4_Devops_engg.docx',\n",
       " '/home/anudeepadi/Documents/Fw__Sample_resumes_/Resume2_Java_full_stack.docx',\n",
       " '/home/anudeepadi/Documents/Fw__Sample_resumes_/Resume_9_cyber_security_.docx',\n",
       " '/home/anudeepadi/Documents/Fw__Sample_resumes_/Resume_12_backend_developer_.docx',\n",
       " '/home/anudeepadi/Documents/Fw__Sample_resumes_/Resume_6_data_engg.docx']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class parse:\n",
    "    def __init__(self, file_list, nlp):\n",
    "        self.file_list = file_list\n",
    "        self.resumes = [textract.process(file_path).decode() for file_path in file_list]\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def extract(self):\n",
    "        ph_list, email_list, dob_list, names_list, location_list, skills_list, skillset_exp = [], [], [], [], [], [], []\n",
    "        for resume in self.resumes:\n",
    "            items = re.finditer(r'\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}', resume)\n",
    "            ph_span = [(resume[item.span()[0]:item.span()[1]], item.span()) for item in items]\n",
    "            ph_list.append(ph_span)\n",
    "            items = re.finditer(r'[\\w\\.-]+@[\\w\\.-]+', resume)\n",
    "            em_span = [(resume[item.span()[0]:item.span()[1]], item.span()) for item in items]\n",
    "            email_list.append(em_span)\n",
    "            result2 = re.finditer(r\"[\\d]{1,2}-[\\d]{1,2}-[\\d]{4}\", resume) # DD-MM-YYYY\n",
    "            dob_span = [(resume[item.span()[0]:item.span()[1]], item.span()) for item in result2]\n",
    "            dob_list.append(dob_span)\n",
    "            doc = self.nlp(resume)\n",
    "            list_ls, list_ns, list_ss = [], [], []\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PERSON':\n",
    "                    es_names = (ent.text, ent.start_char, ent.end_char)\n",
    "                    list_ns.append(es_names)\n",
    "                if ent.label_ in ['GPE', 'LOC']:\n",
    "                    es_locs = (ent.text, ent.start_char, ent.end_char)\n",
    "                    list_ls.append(es_locs) \n",
    "                if ent.label_ in ['PRODUCT', 'ORG']:\n",
    "                    s_span = (ent.text, ent.start_char, ent.end_char)\n",
    "                    list_ss.append(s_span)\n",
    "            location_list.append(list_ls)\n",
    "            names_list.append(list_ns)\n",
    "            skillset_exp.append(list_ss)\n",
    "            noun_chunks = doc.noun_chunks\n",
    "            # tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "            tokens = [token.text for token in doc if token.is_stop]\n",
    "            data = pd.read_csv(\"skills.csv\") \n",
    "            skills = list(data.columns.values)\n",
    "\n",
    "            skillset = []\n",
    "            for token in tokens:\n",
    "                if token.lower() in skills:\n",
    "                    skillset.append(token)\n",
    "\n",
    "            for token in noun_chunks:\n",
    "                token = token.text.lower().strip()\n",
    "                if token in skills:\n",
    "                    skillset.append(token)\n",
    "\n",
    "            st = list(set([i.capitalize() for i in set([i.lower() for i in skillset])]))\n",
    "            skills_list.append(st)\n",
    "        return ph_list, email_list, dob_list, names_list, location_list, skills_list, skillset_exp \n",
    "    \n",
    "    def dictify(self):\n",
    "        dict_ = {}\n",
    "        ph_list, email_list, dob_list, names_list, location_list, skills_list, skillset_exp = self.extract()\n",
    "        for idx, ph, email, dob, name, location, skills, skills_exp in zip(\n",
    "            [file.split('.')[-2].split('/')[-1] for file in self.file_list], \n",
    "            ph_list, \n",
    "            email_list, \n",
    "            dob_list, \n",
    "            names_list, \n",
    "            location_list, \n",
    "            skills_list,\n",
    "            skillset_exp\n",
    "        ):\n",
    "            dict_[idx] = {}\n",
    "            if len(dob):\n",
    "                dict_[idx]['dob'] = dob[0]\n",
    "            else:\n",
    "                dict_[idx]['dob'] = (None, None)\n",
    "            if len(name):\n",
    "                dict_[idx]['name'] = name[0]\n",
    "            else:\n",
    "                dict_[idx]['name'] = (None, None)\n",
    "            if len(email): \n",
    "                dict_[idx]['email'] = email[0]\n",
    "            else:\n",
    "                dict_[idx]['email'] = (None, None)\n",
    "            if len(ph):\n",
    "                dict_[idx]['phone'] = ph[0]\n",
    "            else:\n",
    "                dict_[idx]['phone'] = (None, None)\n",
    "            if len(location):\n",
    "                dict_[idx]['location'] = location[0]\n",
    "            else:\n",
    "                dict_[idx]['location'] = (None, None)\n",
    "            dict_[idx]['skills'] = skills\n",
    "            dict_[idx]['skills_exp'] = skills_exp\n",
    "        return dict_     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "p = parse(file_list, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.96 s, sys: 112 ms, total: 6.07 s\n",
      "Wall time: 6.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ph_list, email_list, dob_list, names_list, location_list, skills_list, ents_ = p.extract()\n",
    "resume_dict = p.dictify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('parsed_output2.json', 'w') as f:\n",
    "    json.dump(resume_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81d08055a2b8858ec83ead00b366ca1806c9068364125977b74f5797f47d76cf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
