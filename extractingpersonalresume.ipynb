{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36e055f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting constants\n",
      "  Downloading constants-0.6.0.tar.gz (5.1 kB)\n",
      "Building wheels for collected packages: constants\n",
      "  Building wheel for constants (setup.py): started\n",
      "  Building wheel for constants (setup.py): finished with status 'done'\n",
      "  Created wheel for constants: filename=constants-0.6.0-py3-none-any.whl size=5473 sha256=b98c50a6d54f3aa7fdc2d753e75a4d6b4a7eb58effb2d76e8bab7ae3622fe605\n",
      "  Stored in directory: c:\\users\\anude\\appdata\\local\\pip\\cache\\wheels\\2c\\f8\\73\\4eea26e0f7d0d170a29e16568c6d2d09bbbbdd169a42637afe\n",
      "Successfully built constants\n",
      "Installing collected packages: constants\n",
      "Successfully installed constants-0.6.0\n"
     ]
    }
   ],
   "source": [
    "#pip install pdfminer.six\n",
    "#pip install doc2text\n",
    "# pip install nltk\n",
    "#pip install spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "# pip install nltk\n",
    "#python -m nltk nltk.download('words')\n",
    "#pip install constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f866db32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl (777.4 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from en-core-web-lg==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\anude\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.2.0\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a11fda99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import constants as cs\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# enc_core_lg\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            resource_manager = PDFResourceManager()\n",
    "            fake_file_handle = io.StringIO()\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "            page_interpreter.process_page(page)\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    for ent in nlp_text.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    resume_text = ''\n",
    "    for page in extract_text_from_pdf(pdf_path):\n",
    "        resume_text += page\n",
    "    return resume_text\n",
    "\n",
    "# extract address\n",
    "# def extract_address(resume_text):\n",
    "#     nlp_text = nlp(resume_text)\n",
    "#     pattern = [{'LOWER': 'address'}, {'IS_DIGIT': True}]\n",
    "    \n",
    "#     matcher.add('ADDRESS', [pattern])\n",
    "    \n",
    "#     matches = matcher(nlp_text)\n",
    "    \n",
    "#     for match_id, start, end in matches:\n",
    "#         span = nlp_text[start:end]\n",
    "#         return span.text\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    EDUCATION = ['BE','B.E.', 'B.E', 'BS','B.S','B.Com','BCA','ME','M.E', 'M.E.', 'M.S','B.com','10','10+2','BTECH', 'B.TECH', 'M.TECH', 'MTECH', 'SSC', 'HSC', 'C.B.S.E','CBSE','ICSE', 'X', 'XII','10th','12th',' 10th',' 12th','Bachelor of Arts in Mathematics','Master of Science in Analytics','Bachelor of Business Administration','Major: Business Management']\n",
    "\n",
    "    nlp_text = nlp(resume_text)\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n",
    "\n",
    "\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    # regex working with +14155827457,+91 9223339060, 614-555-5555, (520) 271-2492, +919619496873 , +91-9619496873  \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n",
    "\n",
    "\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    noun_chunks = nlp(resume_text)\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    skills = list(data.columns.values)\n",
    "    \n",
    "    skillset = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    " \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "               \n",
    "\n",
    "def extract_dob(text):\n",
    "    result1=re.findall(r\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\",text) # 31/12/2001\n",
    "    result2=re.findall(r\"[\\d]{1,2}-[\\d]{1,2}-[\\d]{4}\",text) # 31-12-2001\n",
    "    result3= re.findall(r\"[\\d]{1,2} [ADFJMNOSadfjmnos]\\w* [\\d]{4}\",text) # 31 December 2001\n",
    "    result4=re.findall(r\"([\\d]{1,2})\\.([\\d]{1,2})\\.([\\d]{4})\",text) # 31.12.2001\n",
    "                \n",
    "    l=[result1,result2,result3,result4]\n",
    "    for i in l:\n",
    "        if i==[]:\n",
    "            continue\n",
    "        else:\n",
    "            return i\n",
    "\n",
    "\n",
    "def extract_experience(resume_text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # word tokenization \n",
    "    word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "    # remove stop words and lemmatize  \n",
    "    # filtered_sentence = [w for w in word_tokens if not w in stop_words and wordnet_lemmatizer.lemmatize(w) not in stop_words] \n",
    "    filtered_sentence = [w for w in word_tokens]\n",
    "    sent = nltk.pos_tag(filtered_sentence)\n",
    "    # parse regex\n",
    "    cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "    cs = cp.parse(sent)\n",
    "\n",
    "    ###TODO: afterwards with stanza - stanford's nlp , spacy-stanza\n",
    "    test = []\n",
    "    \n",
    "    for vp in list(cs.subtrees(filter=lambda x: x.label()=='P')):\n",
    "        test.append(\" \".join([i[0] for i in vp.leaves() if len(vp.leaves()) >= 2]))\n",
    "\n",
    "    # Search the word 'experience' in the chunk and then print out the text after it\n",
    "    x = [x[x.lower().index('experience') + 10:] for i, x in enumerate(test) if x and 'experience' in x.lower()]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb52feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\Coding --python\\Resumes\\Anudeep Resume.pdf (2).pdf\"\n",
    "text = ''\n",
    "page = ''\n",
    "for page in extract_text_from_pdf(file_path):\n",
    "    text += ' ' + page\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87ed4837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adiraju Venkata Anudeep ORG\n",
      "Hyderabad GPE\n",
      "India GPE\n",
      "Github Profile - www.github.com/anudeepadi PERSON\n",
      "Linkedin Profile - www.linkedin.com/in/adiraju-venkata-anudeep-7460571b9/ PERSON\n",
      "31/12/2001 CARDINAL\n",
      "daily DATE\n",
      "Hardworking Technical Intern ORG\n",
      "Java ORG\n",
      "SQL ORG\n",
      "1 CARDINAL\n",
      "Big Data ORG\n",
      "Hadoop ORG\n",
      "Ecosystem ORG\n",
      "2 CARDINAL\n",
      "EC2 ORG\n",
      "S3 ORG\n",
      "RDS ORG\n",
      "SQS ORG\n",
      "SNS ORG\n",
      "Route 53 GPE\n",
      "RedShift ORG\n",
      "3 CARDINAL\n",
      "Machine Learning ORG\n",
      "Linear \n",
      "    Regression ORG\n",
      "Logistic Regression, Back Propagated Neural Networks ORG\n",
      "Support Vector  \n",
      "    Machines ORG\n",
      "TensorFlow PRODUCT\n",
      "Matplotlib PERSON\n",
      "Seaborn ORG\n",
      "Shell ORG\n",
      "Git PERSON\n",
      "5 CARDINAL\n",
      "CSS ORG\n",
      "JavaScript ORG\n",
      "Node.js ORG\n",
      "Express.js LOC\n",
      "6 CARDINAL\n",
      "7 CARDINAL\n",
      "Heroku ORG\n",
      "Microsoft Azure ORG\n",
      "GCP ORG\n",
      "Computer Science and Engineering - 2019 - 2023 ORG\n",
      "College - Hyderabad Institute of Technology and Management \n",
      "Affiliated ORG\n",
      "Jawaharlal Nehru Technological University ORG\n",
      "CGPA ORG\n",
      "previous semester DATE\n",
      "6.9 CARDINAL\n",
      "Board - TSBIE ORG\n",
      "69% PERCENT\n",
      "CGPA ORG\n",
      "8.7 CARDINAL\n",
      "10th ORDINAL\n",
      "Bhashyam Public School ORG\n",
      "Board - TSSSC ORG\n",
      "Python ORG\n",
      "Java ORG\n",
      "Location ORG\n",
      "Weather Tracker ORG\n",
      "Python ORG\n",
      "a Complete Blog Management Site ORG\n",
      "MERN Stack ORG\n",
      "OCR Management ORG\n",
      "AWS ORG\n",
      "Google Data Analytics Professional Certificate - Coursera \n",
      "Completed ORG\n",
      "eight CARDINAL\n",
      "Data Analytics ORG\n",
      "SQL ORG\n",
      "Tableau ORG\n",
      "R.\n",
      "Preparing PERSON\n",
      "1 CARDINAL\n",
      "2 CARDINAL\n",
      "3 CARDINAL\n",
      "1 CARDINAL\n",
      "1 CARDINAL\n",
      "PUT ORG\n",
      "3 CARDINAL\n",
      "1st ORDINAL\n",
      "Hyderabad Institute of Technology and Management Hackathon ORG\n",
      "4 CARDINAL\n",
      "AWS ORG\n",
      "1 CARDINAL\n",
      "2 CARDINAL\n",
      "JPMorgan Chase & Co. ORG\n",
      "Software Engineering Virtual Experience Virtual ORG\n",
      "Forage ORG\n",
      "1 CARDINAL\n",
      "2 CARDINAL\n",
      "JPMorgan Chase ORG\n",
      "Deloitte Virtual Internship ORG\n",
      "Technology Consulting Virtual Program ORG\n",
      "1 CARDINAL\n",
      "2 CARDINAL\n",
      "3 CARDINAL\n",
      "4 CARDINAL\n",
      "Accenture Virtual Developer Experience Program Participant ORG\n",
      "2021 DATE\n",
      "Developer Virtual Program ORG\n",
      "1 CARDINAL\n",
      "2 CARDINAL\n",
      "3 CARDINAL\n",
      "Securing ORG\n",
      "SDLC ORG\n"
     ]
    }
   ],
   "source": [
    "dictionary ={\n",
    "    \"Name\" : extract_name(text),\n",
    "    \"Mobile Number\" : extract_mobile_number(text),\n",
    "    \"Email\" : extract_email(text),\n",
    "    \"Skills\" : extract_skills(text),\n",
    "    # \"Address\" : extract_address(text),\n",
    "    \"Education\" : extract_education(text),\n",
    "    \"Date of Birth\" : extract_dob(text),\n",
    "    \"Experience\" : extract_experience(text)\n",
    "    # \"Achievements\" : extract_achievements(text),\n",
    "    # \"Certifications\" : extract_certifications(text),\n",
    "    # \"Languages\" : extract_languages(text),\n",
    "    # \"References\" : extract_references(text),\n",
    "    # \"Interests\" : extract_interests(text),\n",
    "    # \"Additional Information\" : extract_additional_information(text)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19804a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(dictionary, indent = 4)\n",
    "with open(\"extracted_details1.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "# print(extract_name(text))\n",
    "# print(extract_mobile_number(text))\n",
    "# print(extract_email(text))\n",
    "# print(extract_dob(text))\n",
    "# print(extract_skills(text))\n",
    "# print(extract_experience(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55493799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
