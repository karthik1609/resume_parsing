{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36e055f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anude\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\anude\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\anude\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anude\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anude\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\anude\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "#pip install pdfminer.six\n",
    "#pip install doc2text\n",
    "# pip install nltk\n",
    "#pip install spacy\n",
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11fda99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import io\n",
    "import nltk\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            resource_manager = PDFResourceManager()\n",
    "            fake_file_handle = io.StringIO()\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "            page_interpreter.process_page(page)\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n",
    "\n",
    "\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    noun_chunks = nlp(resume_text)\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    skills = list(data.columns.values)\n",
    "    \n",
    "    skillset = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    " \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "               \n",
    "# def extract_dob(dob):\n",
    "#     dob = re.findall(re.compile(r'([0-9]{2}[/]{1}[0-9]{2}[/]{1}[0-9]{4})'), dob)\n",
    "#     if dob:\n",
    "#         try:\n",
    "#             return datetime.strptime(dob[0], '%d-%m-%Y').strftime('%d-%m-%Y')\n",
    "#         except ValueError:\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb52feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/anude/Downloads/Anudeep Resume.pdf (1).pdf'\n",
    "text = ''\n",
    "page = ''\n",
    "for page in extract_text_from_pdf(file_path):\n",
    "    text += ' ' + page\n",
    "    # load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ed4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary ={\n",
    "    \"Name\" : extract_name(text),\n",
    "    \"Mobile Number\" : extract_mobile_number(text),\n",
    "    \"Email\" : extract_email(text),\n",
    "    \"Skills\" : extract_skills(text)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19804a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(dictionary, indent = 4)\n",
    "with open(\"extracted_details.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "# print(extract_name(text))\n",
    "# print(extract_mobile_number(text))\n",
    "# print(extract_email(text))\n",
    "# print(extract_dob(text))\n",
    "# print(extract_skills(text))\n",
    "# print(extract_experience(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1326d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
