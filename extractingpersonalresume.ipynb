{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36e055f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting constants\n",
      "  Downloading constants-0.6.0.tar.gz (5.1 kB)\n",
      "Building wheels for collected packages: constants\n",
      "  Building wheel for constants (setup.py): started\n",
      "  Building wheel for constants (setup.py): finished with status 'done'\n",
      "  Created wheel for constants: filename=constants-0.6.0-py3-none-any.whl size=5473 sha256=b98c50a6d54f3aa7fdc2d753e75a4d6b4a7eb58effb2d76e8bab7ae3622fe605\n",
      "  Stored in directory: c:\\users\\anude\\appdata\\local\\pip\\cache\\wheels\\2c\\f8\\73\\4eea26e0f7d0d170a29e16568c6d2d09bbbbdd169a42637afe\n",
      "Successfully built constants\n",
      "Installing collected packages: constants\n",
      "Successfully installed constants-0.6.0\n"
     ]
    }
   ],
   "source": [
    "#pip install pdfminer.six\n",
    "#pip install doc2text\n",
    "# pip install nltk\n",
    "#pip install spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "# pip install nltk\n",
    "#python -m nltk nltk.download('words')\n",
    "#pip install constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a11fda99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import constants as cs\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# enc_core_lg\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            resource_manager = PDFResourceManager()\n",
    "            fake_file_handle = io.StringIO()\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "            page_interpreter.process_page(page)\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    resume_text = ''\n",
    "    for page in extract_text_from_pdf(pdf_path):\n",
    "        resume_text += page\n",
    "    return resume_text\n",
    "\n",
    "# extract address\n",
    "def extract_address(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    pattern = [{'LOWER': 'address'}, {'IS_DIGIT': True}]\n",
    "    \n",
    "    matcher.add('ADDRESS', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    EDUCATION = ['BE','B.E.', 'B.E', 'BS','B.S','B.Com','BCA','ME','M.E', 'M.E.', 'M.S','B.com','10','10+2','BTECH', 'B.TECH', 'M.TECH', 'MTECH', 'SSC', 'HSC', 'C.B.S.E','CBSE','ICSE', 'X', 'XII','10th','12th',' 10th',' 12th','Bachelor of Arts in Mathematics','Master of Science in Analytics','Bachelor of Business Administration','Major: Business Management']\n",
    "\n",
    "\n",
    "    nlp_text = nlp(resume_text)\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n",
    "\n",
    "\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n",
    "\n",
    "\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    noun_chunks = nlp(resume_text)\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    skills = list(data.columns.values)\n",
    "    \n",
    "    skillset = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    " \n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "               \n",
    "\n",
    "def extract_dob(text):\n",
    "    #TODO: add other regular expressions\n",
    "    result1=re.findall(r\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\",text)\n",
    "    result2=re.findall(r\"[\\d]{1,2}-[\\d]{1,2}-[\\d]{4}\",text)           \n",
    "    result3= re.findall(r\"[\\d]{1,2} [ADFJMNOSadfjmnos]\\w* [\\d]{4}\",text)\n",
    "    result4=re.findall(r\"([\\d]{1,2})\\.([\\d]{1,2})\\.([\\d]{4})\",text)\n",
    "                \n",
    "    l=[result1,result2,result3,result4]\n",
    "    for i in l:\n",
    "        if i==[]:\n",
    "            continue\n",
    "        else:\n",
    "            return i\n",
    "\n",
    "def extract_experience(resume_text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # word tokenization \n",
    "    word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "    # remove stop words and lemmatize  \n",
    "    # filtered_sentence = [w for w in word_tokens if not w in stop_words and wordnet_lemmatizer.lemmatize(w) not in stop_words] \n",
    "    filtered_sentence = [w for w in word_tokens]\n",
    "    sent = nltk.pos_tag(filtered_sentence)\n",
    "    print(sent)\n",
    "\n",
    "    # parse regex\n",
    "    cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "    cs = cp.parse(sent)\n",
    "\n",
    "    ###TODO: afterwards with stanza - stanford's nlp , spacy-stanza\n",
    "    test = []\n",
    "    \n",
    "    for vp in list(cs.subtrees(filter=lambda x: x.label()=='P')):\n",
    "        test.append(\" \".join([i[0] for i in vp.leaves() if len(vp.leaves()) >= 2]))\n",
    "\n",
    "    # Search the word 'experience' in the chunk and then print out the text after it\n",
    "    x = [x[x.lower().index('experience') + 10:] for i, x in enumerate(test) if x and 'experience' in x.lower()]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bd53199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('skills', 'NNS'), ('include', 'VBP'), ('Python', 'NNP'), ('and', 'CC'), ('Java', 'NNP')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'My skills include Python and Java'\n",
    "extract_experience(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb52feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:/Coding --python/Resumes/atul sharma.pdf'\n",
    "text = ''\n",
    "page = ''\n",
    "for page in extract_text_from_pdf(file_path):\n",
    "    text += ' ' + page\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a3cb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "sample_dob = 'October 11, 1961'\n",
    "extract_dob(sample_dob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ed4837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Atul Sharma', 'Mobile Number': '+919223339060', 'Email': 'Atuls61@gmail.com', 'Skills': ['Regulatory', 'Process', 'Benchmarking', 'Inventory', 'Vendors', 'Erp', 'Reporting', 'Oracle', 'Tax', 'Compliance', 'Accounting', 'Proposal', 'Banking', 'Operations', 'Strategy', 'Mis', 'Sourcing', 'Modeling', 'Analysis', 'Finance', 'Negotiation', 'Documentation', 'Sap', 'Budgeting', 'Controls'], 'Experience': [' February']}\n"
     ]
    }
   ],
   "source": [
    "dictionary ={\n",
    "    \"Name\" : extract_name(text),\n",
    "    \"Mobile Number\" : extract_mobile_number(text),\n",
    "    \"Email\" : extract_email(text),\n",
    "    \"Skills\" : extract_skills(text),\n",
    "    # \"Address\" : extract_address(text),\n",
    "    \"Education\" : extract_education(text),\n",
    "    \"Date of Birth\" : extract_dob(text),\n",
    "    \"Experience\" : extract_experience(text)\n",
    "    # \"Projects\" : extract_projects(text),\n",
    "    # \"Achievements\" : extract_achievements(text),\n",
    "    # \"Certifications\" : extract_certifications(text),\n",
    "    # \"Languages\" : extract_languages(text),\n",
    "    # \"References\" : extract_references(text),\n",
    "    # \"Interests\" : extract_interests(text),\n",
    "    # \"Additional Information\" : extract_additional_information(text)\n",
    "}\n",
    "dictionary = {k:v for k,v in dictionary.items() if v}\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19804a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(dictionary, indent = 4)\n",
    "with open(\"extracted_details1.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "# print(extract_name(text))\n",
    "# print(extract_mobile_number(text))\n",
    "# print(extract_email(text))\n",
    "# print(extract_dob(text))\n",
    "# print(extract_skills(text))\n",
    "# print(extract_experience(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce9de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
