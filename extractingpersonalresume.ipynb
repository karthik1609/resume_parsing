{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pdfminer.six\n",
    "! pip install doc2text\n",
    "! pip install nltk\n",
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm\n",
    "! pip install nltk\n",
    "! python -m nltk nltk.download('words')\n",
    "! pip install constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a11fda99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import constants as cs\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# enc_core_lg\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            resource_manager = PDFResourceManager()\n",
    "            fake_file_handle = io.StringIO()\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "            page_interpreter.process_page(page)\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    resume_text = ''\n",
    "    for page in extract_text_from_pdf(pdf_path):\n",
    "        resume_text += page\n",
    "    return resume_text\n",
    "\n",
    "\n",
    "# def extract_education(resume_text):\n",
    "#     STOPWORDS = set(stopwords.words('english'))\n",
    "#     EDUCATION = ['BE','B.E.', 'B.E', 'BS','B.S','B.Com','BCA','ME','M.E', 'M.E.', 'M.S','B.com','10','10+2','BTECH', 'B.TECH', 'M.TECH', 'MTECH', 'SSC', 'HSC', 'C.B.S.E','CBSE','ICSE', 'X', 'XII','10th','12th',' 10th',' 12th','Bachelor of Arts in Mathematics','Master of Science in Analytics','Bachelor of Business Administration','Major: Business Management']\n",
    "\n",
    "#     nlp_text = nlp(resume_text)\n",
    "#     nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "#     edu = {}\n",
    "#     for index, text in enumerate(nlp_text):\n",
    "#         for tex in text.split():\n",
    "#             tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "#             if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "#                 edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "#     education = []\n",
    "#     for key in edu.keys():\n",
    "#         year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "#         if year:\n",
    "#             education.append((key, ''.join(year[0])))\n",
    "#         else:\n",
    "#             education.append(key)\n",
    "#     return education\n",
    "\n",
    "\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    # regex working with +14155827457,+91 9223339060, 614-555-5555, (520) 271-2492, +919619496873 , +91-9619496873  \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n",
    "\n",
    "\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "# def extract_skills(resume_text):\n",
    "# nlp_text = nlp(resume_text)\n",
    "# noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "# def extract_skills(resume_text):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     nlp_text = nlp(resume_text)\n",
    "#     noun_chunks = nlp_text.noun_chunks\n",
    "    \n",
    "#     # tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "#     tokens = [token.text for token in nlp_text if token.is_stop]\n",
    "\n",
    "#     data = pd.read_csv(\"skills.csv\") \n",
    "#     skills = list(data.columns.values)\n",
    "    \n",
    "#     skillset = []\n",
    "#     for token in tokens:\n",
    "#         if token.lower() in skills:\n",
    "#             skillset.append(token)\n",
    " \n",
    "#     for token in noun_chunks:\n",
    "#         token = token.text.lower().strip()\n",
    "#         if token in skills:\n",
    "#             skillset.append(token)\n",
    "    \n",
    "#     return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n",
    "def extract_dob(text):\n",
    "    result1=re.findall(r\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\",text) # 31/12/2001\n",
    "    result2=re.findall(r\"[\\d]{1,2}-[\\d]{1,2}-[\\d]{4}\",text) # 31-12-2001\n",
    "    #TODO: result3 nd, st, rd, th\n",
    "    result3= re.findall(r\"[\\d]{1,2} [ADFJMNOSadfjmnos]\\w* [\\d]{4}\",text) # 31 December 2001\n",
    "    result4=re.findall(r\"([\\d]{1,2})\\.([\\d]{1,2})\\.([\\d]{4})\",text) # 31.12.2001\n",
    "                \n",
    "    l=[result1,result2,result3,result4]\n",
    "    for i in l:\n",
    "        if i==[]:\n",
    "            continue\n",
    "        else:\n",
    "            return i\n",
    "\n",
    "# def get_experience(document):\n",
    "#     pattern1 = re.compile(r'(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)(\\s|\\S)(\\d{2,4}).*(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)(\\s|\\S)(\\d{2,4})')\n",
    "#     pattern2 = re.compile(r'(\\d{2}(.|..)\\d{4}).{1,4}(\\d{2}(.|..)\\d{4})')\n",
    "#     pattern3 = re.compile(r'(\\d{2}(.|..)\\d{4}).{1,4}(present)')\n",
    "#     pattern4 = re.compile(r'(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)(\\s|\\S)(\\d{2,4}).*(present)')\n",
    "#     patterns = [pattern1, pattern2, pattern3, pattern4]\n",
    "#     experience = []\n",
    "#     for index, line in enumerate(document):\n",
    "#         for pattern in patterns:\n",
    "#             exp = pattern.findall(line)\n",
    "#             if len(exp) > 0:\n",
    "#                 experience.append(document[index:index+4])\n",
    "#     return (experience)\n",
    "    \n",
    "def extract_location(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['GPE', 'LOC']:\n",
    "            ls = list(ent.text)\n",
    "    string = ''.join(ls)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1a256205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import relativedelta\n",
    "import constants as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "211d62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_sections_grad(text):\n",
    "    text_split = [i.strip() for i in text.split('\\n')]\n",
    "    entities = {}\n",
    "    key = False\n",
    "    for phrase in text_split:\n",
    "        if len(phrase) == 1:\n",
    "            p_key = phrase\n",
    "        else:\n",
    "            p_key = set(phrase.lower().split()) & set(cs.RESUME_SECTIONS_GRAD)\n",
    "        try:\n",
    "            p_key = list(p_key)[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        if p_key in cs.RESUME_SECTIONS_GRAD:\n",
    "            entities[p_key] = []\n",
    "            key = p_key\n",
    "        elif key and phrase.strip():\n",
    "            entities[key].append(phrase)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fb52feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\Coding --python\\Resumes\\SGresume-1.pdf\"\n",
    "text = ''\n",
    "page = ''\n",
    "for page in extract_text_from_pdf(file_path):\n",
    "    text += ' ' + page\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "87ed4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary ={\n",
    "    \"Name\" : extract_name(text),\n",
    "    \"Mobile Number\" : extract_mobile_number(text),\n",
    "    \"Email\" : extract_email(text),\n",
    "    # \"Address\" : extract_address(text),\n",
    "    \"Date of Birth\" : extract_dob(text),\n",
    "    # \"Location\" : extract_location(text),\n",
    "    \"Entities\" : extract_entity_sections_grad(text)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0fea143f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experience', 'projects', 'education', 'leadership', 'skills', 'interests']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyslist = list(dictionary[\"Entities\"].keys())\n",
    "keyslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d91f48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expereicelist = dictionary[\"Entities\"][\"experience\"]\n",
    "skillslist = dictionary[\"Entities\"][\"skills\"]\n",
    "projectlist = dictionary[\"Entities\"][\"projects\"]\n",
    "educationlist = dictionary[\"Entities\"][\"education\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "82d3906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting entities from dictionary\n",
    "del dictionary[\"Entities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d4761df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {\n",
    "    \"experience\" : expereicelist,\n",
    "    \"skills\" : skillslist,\n",
    "    \"projects\" : projectlist,\n",
    "    \"education\" : educationlist\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f9f5c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = dictionary | dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "19804a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(z, indent = 4, ensure_ascii=False)\n",
    "with open(\"extracted_details1.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "# print(extract_name(text))\n",
    "# print(extract_mobile_number(text))\n",
    "# print(extract_email(text))\n",
    "# print(extract_dob(text))\n",
    "# print(extract_skills(text))\n",
    "# print(extract_experience(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98f0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
